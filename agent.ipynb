{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from snake import Snake, visualize\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR=0.001\n",
    "\n",
    "window_x = 720\n",
    "window_y = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(((((7 & (1 << np.arange(4)))) > 0).astype(int))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self) -> None:\n",
    "        self.game=Snake(window_x,window_y)\n",
    "        \n",
    "        self.n_games=0\n",
    "        self.epsilon=0\n",
    "        self.gamma=0.9\n",
    "        self.memory=deque(maxlen=MAX_MEMORY)\n",
    "        \n",
    "        self.model = Linear_QNet(len(self.get_state()),256,4)\n",
    "        self.trainer = QTrainer(self.model,lr=LR,gamma=self.gamma)\n",
    "    \n",
    "    def get_state(self):\n",
    "        \n",
    "        returned_body_size = 10\n",
    "        mx = int(np.ceil(np.log2(self.game.wx/10)))\n",
    "        my = int(np.ceil(np.log2(self.game.wy/10)))\n",
    "        state = []\n",
    "        \n",
    "        for n_of_body in range(returned_body_size):\n",
    "            if n_of_body<len(self.game.snake_body):\n",
    "                body_block = self.game.snake_body[n_of_body]\n",
    "                bbx = ((((int(body_block[0]/10) & (1 << np.arange(mx)))) > 0).astype(int))[::-1]\n",
    "                bby = ((((int(body_block[1]/10) & (1 << np.arange(my)))) > 0).astype(int))[::-1]\n",
    "            else:\n",
    "                bbx = np.zeros((mx,),dtype=int)\n",
    "                bby = np.zeros((my,),dtype=int)\n",
    "                \n",
    "            state=np.concatenate((state,bbx,bby))\n",
    "            \n",
    "        fbx = ((((int(self.game.fruit_position[0]/10) & (1 << np.arange(mx)))) > 0).astype(int))[::-1]\n",
    "        fby = ((((int(self.game.fruit_position[1]/10) & (1 << np.arange(my)))) > 0).astype(int))[::-1]\n",
    "        state = np.concatenate((state,fbx,fby))\n",
    "        \n",
    "        \n",
    "        return np.array(state, dtype=int)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        self.epsilon= 80 - self.n_games\n",
    "        final_move = [0,0,0,0]\n",
    "        \n",
    "        if random.randint(0,200)<self.epsilon:\n",
    "            move = random.randint(0,3)\n",
    "            final_move[move]=1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move]=1\n",
    "        \n",
    "        return final_move\n",
    "    \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory)>BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            mini_sample=self.memory\n",
    "            \n",
    "        state_olds, moves, rewards, state_news, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(state_olds, moves, rewards, state_news, dones)\n",
    "    \n",
    "    def train_short_memory(self, state_old, move, reward, state_new, done):\n",
    "            self.trainer.train_step(state_old, move, reward, state_new, done)\n",
    "        \n",
    "    def remember(self, state_old, move, reward, state_new, done):\n",
    "            self.memory.append((state_old, move, reward, state_new, done))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game=Snake(window_x,window_y)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "ag = Agent()\n",
    "print(ag.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent):\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    \n",
    "    while k<1000:\n",
    "        k+=1\n",
    "        state_old = agent.get_state()\n",
    "        move = agent.get_action(state_old)\n",
    "        \n",
    "        move_ = 'UP'\n",
    "        \n",
    "        if move[1]==1:\n",
    "            move_ = 'DOWN'\n",
    "        if move[2]==1:\n",
    "            move_ = 'RIGHT'\n",
    "        if move[3]==1:\n",
    "            move_ = 'LEFT'\n",
    "        \n",
    "        \n",
    "        \n",
    "        reward = agent.game.step(move_)\n",
    "        \n",
    "        state_new = agent.get_state()\n",
    "        \n",
    "        agent.train_short_memory(state_old, move, reward, state_new, agent.game.game_over)\n",
    "        agent.remember(state_old, move, reward, state_new, agent.game.game_over)\n",
    "        \n",
    "        \n",
    "        if agent.game.game_over:\n",
    "            score = agent.game.score\n",
    "            \n",
    "            agent.reset()\n",
    "            agent.n_games+=1\n",
    "            agent.train_long_memory()\n",
    "            \n",
    "            if score>record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "            \n",
    "            \n",
    "            print(k,'Game', agent.n_games,'Score',score,'Record:',record)\n",
    "            \n",
    "            # plot_scores.append(score)\n",
    "            # total_score+=score\n",
    "            # plot_mean_scores.append(total_score/agent.n_games)\n",
    "            # plot(plot_scores, plot_mean_scores)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 Game 1 Score 0 Record: 0\n",
      "303 Game 2 Score 0 Record: 0\n",
      "396 Game 3 Score 0 Record: 0\n",
      "608 Game 4 Score 0 Record: 0\n",
      "910 Game 5 Score 0 Record: 0\n",
      "992 Game 6 Score 0 Record: 0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "train(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visualize(agent)\n",
      "File \u001b[0;32m~/Documents/neural network/snake_ai/snake.py:227\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m    224\u001b[0m pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    226\u001b[0m \u001b[39m# Frame Per Second /Refresh Rate\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m fps\u001b[39m.\u001b[39mtick(snake_speed)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "visualize(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
